{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mKK4Rd8vhPS"
      },
      "source": [
        "#  **ICT303 - Assignment 1**\n",
        "\n",
        "**Your name: Lim Wen Chao**\n",
        "\n",
        "**Student ID: 34368872**\n",
        "\n",
        "**Email: CT0360379@kaplan.edu.sg**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by2yigAYoub0"
      },
      "source": [
        "## **1. Description**\n",
        "\n",
        "We would like to develop, using Multilayer Perceptron (MLP), a computer program that takes images of handwritten text, finds the written characters in the image and displays the written characters.\n",
        "\n",
        "To achieve this, we will proceed in steps:\n",
        "\n",
        "1. Develop and train an MLP for the recognition of handwritten characters from images. In the first instance, the images are assumed to contain only one handwritten.\n",
        "2. Train and test the MLP, and evaluate its performance by using loss curves and proper accuracy/performance measures\n",
        "3. Improve the performance of the MLP by tuning its hyper parameters.\n",
        "4. Extend the program you developed to localize (detect) and recognize handwritten characters in an image that contains multiple handwritten characters.\n",
        "\n",
        "For this purpose, we will use the following dataset for training, validation and testing: https://www.kaggle.com/datasets/dhruvildave/english-handwritten-characters-dataset.\n",
        "\n",
        "You are required to justify every design choice. Justifications should be theoretical and validated with experiments.\n",
        "\n",
        "It is important that you start as earlier as possible. Coding is usually easy. However, training neural networks and tuning its hyper-parameters takes time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9y_vBKmcS3L"
      },
      "source": [
        "##**2. Marking Guide**##\n",
        "\n",
        "- The overal structure of the program - it should follow the structure we used so far in the labs **[30 Marks]**. This includes:\n",
        "  - A class that defines the network architecture that extends the class `nn.Module`. It should have a constructor method (`__init__()`) and a forward function (`forward()`)\n",
        "  - The Trainer class\n",
        "  - A main function\n",
        "\n",
        "- Training working and running on GPU **[10 marks]**\n",
        "\n",
        "- Curves for training loss and validation loss plotted and training stopped when the network starts to overfit (i.e., when the validation loss starts to increase). You must use TensorBoard to visualize curves and monitor performance **[10 marks]**\n",
        "\n",
        "- Testing code properly working. **[10 marks]**\n",
        "\n",
        "- Hyper parameters finetuned and the best ones selected. **[10 marks]**\n",
        "\n",
        "- Quality of the dicussions **[20 marks]**: did the student discuss various design choices, including the hyperparamters or any choices they made to improve the performance? Any design choice should be properly justified.\n",
        "\n",
        "- Extension to the localization of the characters **[10 marks]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPY9Ha_3qs68"
      },
      "source": [
        "## **3. What to submit**\n",
        "\n",
        "You need to upload to LMS the notebook as well as a folder that contains the .py files you created. All classes should be implemented in .py files. The notebook will sever as a documentation of your work as well as the codes that demonstrated the training, validation and testing of your MLP models that you created.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8fbb7JHaMo1"
      },
      "source": [
        "# **Import Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5528HYH-ZKtc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85fb9c3-4ce6-46db-87ef-7356ada8c2d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "# Importing all dependencies\n",
        "import os # for some OS ops\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "# Load openCV to be used to localise individual characters in an image\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ddVuMrvSLdR"
      },
      "source": [
        "# **Downloading and Unzipping Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYgc4DcAfo6_"
      },
      "source": [
        "Upload the downloaded archive.zip file onto colab before running the unzip command below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "91QZ_1FEbXb6"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/archive'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCxWgPbjN89e"
      },
      "source": [
        "# **Splitting Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using random split to devide up the available images for training, validation and testing.\n",
        "\n",
        "I have decided to manually split the images to ensure that all training are done using the same images. Additionally, it will also prevent the scenario where the training dataset might not have any of a character due to the random split. This will cause the model to not get trained on that character, making it impossible for it to recognise the character.\n",
        "\n",
        "For my manual split, I have partitioned the images to have 40 for traing, 10 for validation and 5 for testing."
      ],
      "metadata": {
        "id": "mJAGIpQJWUtp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kbb0jpLSOCea"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            self.df = data\n",
        "        else:\n",
        "            self.df = pd.read_csv(data)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create a dictionary that maps each label to a unique integer\n",
        "        labels = sorted(self.df['label'].unique())\n",
        "        self.label_to_int = {label: i for i, label in enumerate(labels)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = self.df.loc[idx, 'image']\n",
        "        label = self.df.loc[idx, 'label']\n",
        "        image = Image.open(f\"{rootPath}/{img_name}\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label to tensor\n",
        "        label = torch.tensor(self.label_to_int[label], dtype=torch.long)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def get_dataset(self, dataset_type):\n",
        "        # Extract imgNum from filename\n",
        "        self.df['imgNum'] = self.df['image'].apply(lambda x: int(x.split('-')[1].split('.')[0]))\n",
        "\n",
        "        if dataset_type == 'train':\n",
        "            filtered_df = self.df[(self.df['imgNum'] >= 1) & (self.df['imgNum'] <= 40)].reset_index(drop=True)\n",
        "        elif dataset_type == 'validate':\n",
        "            filtered_df = self.df[(self.df['imgNum'] >= 41) & (self.df['imgNum'] <= 50)].reset_index(drop=True)\n",
        "        elif dataset_type == 'test':\n",
        "            filtered_df = self.df[(self.df['imgNum'] >= 51) & (self.df['imgNum'] <= 55)].reset_index(drop=True)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid dataset type. Choose 'train' or 'test'.\")\n",
        "\n",
        "        # Create a new instance of ImageDataset with the filtered dataframe\n",
        "        return ImageDataset(filtered_df, self.transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaT5g7NZu68w"
      },
      "source": [
        "# **Defining the MLP neural network**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the MLP model, the loss function I am using is cross entropy loss. Cross entropy loss function is normally used for classification problems such as this.\n",
        "\n",
        "In the context of pyTorch, CrossEntropyLoss is used for multi-class classification problem where each test data would belong in a single class only.\n",
        "\n",
        "BCELoss, or Binary Cross Entropy, is another type of cross entropy loss function but is used for binary classification problem where each test data can only be one of two possible classes.\n",
        "\n",
        "BCELoss could also be used for multi-label classification problems where each test data could be multiple classes at the same time.\n",
        "\n",
        "However, for our use case, I believe CrossEntropyLoss would be the best choice as each image/character in an image, should only belong to one class.\n",
        "\n"
      ],
      "metadata": {
        "id": "tiLYYXwIjxzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the optimizer algorithm, Adam is the most commonly used. It uses adaptive learning rates and can be effective even with little tuning of learning rates. This makes the algorithm effective even when used by someone inexperienced in tuning the hyperparameters.\n",
        "\n",
        "On the flip side, SGD is one of the more basic algorithm and only updates model based on the direction of the negative gradient.\n",
        "\n",
        "For our use case, I would prefer to use Adam, the more popular and effective optimizer over other optimizers."
      ],
      "metadata": {
        "id": "Ytx2YV2Xp5hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the activation function, the choices considered where ReLU and Sigmoid.\n",
        "\n",
        "ReLU is one of the more commonly used activation function. It outputs the input directly if it is positive but outputs zero otherwise.\n",
        "\n",
        "Sigmoid outputs its inputs in the range of 0 to 1 and can be interpreted as probability.\n",
        "\n",
        "The advantages of ReLU is that it is computationally efficient since it just outputs 0 when the input is not positive.\n",
        "\n",
        "Addtionally, Sigmoid suffers from vanishing gradient problem where, the very high or low values have almost no gradient and will fail to update the weights effectively.\n",
        "\n",
        "Because of Sigmoid function's disadvantage, ReLU serves as a more effective activation function for our hidden layers."
      ],
      "metadata": {
        "id": "q1RZi82Ky2Jc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sO_ndOTbBREN"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self, inputSize=1200 * 900, outputSize=62, lr=0.01):\n",
        "    super().__init__()\n",
        "    # Define the layers of the network\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),  # Flatten the input tensor\n",
        "      nn.Linear(inputSize, 256),  # Linear layer with 256 output\n",
        "      nn.ReLU(),  # ReLU activation function\n",
        "      nn.Linear(256, 128),  # Linear layer with 128 output\n",
        "      nn.ReLU(),  # ReLU activation function\n",
        "      nn.Linear(128, outputSize),\n",
        "    )\n",
        "    # Setting the learning rate\n",
        "    self.lr = lr\n",
        "\n",
        "  ## The forward step\n",
        "  def forward(self, X):\n",
        "    # Computes the output given the input X\n",
        "    return self.layers(X)\n",
        "\n",
        "  ## The loss function - Here, we will use Cross Entropy Loss\n",
        "  def loss(self, y_hat, y):\n",
        "    fn = nn.CrossEntropyLoss()\n",
        "    return fn(y_hat, y)\n",
        "\n",
        "  ## The optimization algorithm\n",
        "  #  Let's this time use Adam, which is the most commonly used optimizer in neural networks\n",
        "  def configure_optimizers(self):\n",
        "    # update network weights iteratively based on the training data.\n",
        "    return torch.optim.Adam(self.parameters(), self.lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alG_eiLsvAmz"
      },
      "source": [
        "# **The training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "knK8NIpGBaB3"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "  '''\n",
        "    Trainer class for training and validating a model.\n",
        "  '''\n",
        "  def __init__(self, n_epochs = 3):\n",
        "    self.max_epochs = n_epochs  # Maximum number of training epochs\n",
        "    self.writer = SummaryWriter('./runs/train')  # Initialize the TensorBoard writer\n",
        "\n",
        "  def fit(self, model, train_data, val_data):\n",
        "    '''\n",
        "      Function to train and validate the model.\n",
        "    '''\n",
        "    self.train_data = train_data  # Training data\n",
        "    self.val_data = val_data  # Validation data\n",
        "\n",
        "    # Configure the optimizer\n",
        "    self.optimizer = model.configure_optimizers()\n",
        "    self.model = model  # The model to train\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch in range(self.max_epochs):\n",
        "        print('Epoch ',epoch, ': ')\n",
        "\n",
        "        # Train for one epoch and get training loss\n",
        "        train_loss = self.fit_epoch(epoch)\n",
        "        print('training loss: ', train_loss)\n",
        "\n",
        "        # Validate for one epoch and get validation loss\n",
        "        val_loss = self.validate_epoch(epoch)\n",
        "        print('validation loss: ', val_loss)\n",
        "        print()\n",
        "\n",
        "        # Log the loss to TensorBoard\n",
        "        self.writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
        "\n",
        "    print(\"Training process has finished\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), rootPath + 'model.pth')\n",
        "\n",
        "    self.writer.close()  # Close the TensorBoard writer\n",
        "\n",
        "  def fit_epoch(self, epoch):\n",
        "    '''\n",
        "      Function to train the model for one epoch.\n",
        "    '''\n",
        "    current_loss = 0.0  # Initialize current loss\n",
        "    correct = 0  # Initialize count of correct predictions\n",
        "    total = 0  # Initialize total count of predictions\n",
        "    self.model.train()  # Set the model to training mode\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(self.train_data):\n",
        "        # Get input and its corresponding groundtruth output\n",
        "        inputs, target = data\n",
        "\n",
        "        # Clear gradient buffers\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Get output from the model, given the inputs\n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        # Get the predicted labels\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Get loss for the predicted output\n",
        "        loss = self.model.loss(outputs, target)\n",
        "\n",
        "        # Get gradients w.r.t the parameters of the model\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update loss\n",
        "        current_loss += loss.item()\n",
        "\n",
        "        # Update total count of predictions\n",
        "        total += target.size(0)\n",
        "\n",
        "        # Update count of correct predictions\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total\n",
        "    print('training accuracy: ', accuracy*100)\n",
        "\n",
        "    # Log the accuracy to TensorBoard\n",
        "    self.writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
        "\n",
        "    return current_loss / len(self.train_data)\n",
        "\n",
        "  def validate_epoch(self, epoch):\n",
        "    '''\n",
        "      Function to validate the model for one epoch.\n",
        "    '''\n",
        "    val_loss = 0.0  # Initialize validation loss\n",
        "    correct = 0  # Initialize count of correct predictions\n",
        "    total = 0  # Initialize total count of predictions\n",
        "    self.model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Temporarily turn off gradient descent\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the DataLoader for validation data\n",
        "        for i, data in enumerate(self.val_data):\n",
        "            # Get input and its corresponding groundtruth output\n",
        "            inputs, target = data\n",
        "\n",
        "            # Get output from the model, given the inputs\n",
        "            outputs = self.model(inputs)\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Get loss for the predicted output\n",
        "            loss = self.model.loss(outputs, target)\n",
        "\n",
        "            # Update validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Update total count of predictions\n",
        "            total += target.size(0)\n",
        "\n",
        "            # Update count of correct predictions\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total\n",
        "    print('validation accuracy: ', accuracy*100)\n",
        "\n",
        "    # Log the accuracy to TensorBoard\n",
        "    self.writer.add_scalar('Accuracy/val', accuracy, epoch)\n",
        "\n",
        "    return val_loss / len(self.val_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YmFRPc5eLVJ"
      },
      "source": [
        "# **The Testing Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WEqEONo2eT1P"
      },
      "outputs": [],
      "source": [
        "class Tester:\n",
        "  '''\n",
        "    Tester class for testing a model.\n",
        "  '''\n",
        "  def __init__(self, model):\n",
        "    self.model = model  # The model to test\n",
        "    self.writer = SummaryWriter('./runs/test')  # Initialize the TensorBoard writer\n",
        "\n",
        "  def test(self, test_data):\n",
        "    '''\n",
        "      Function to test the model.\n",
        "    '''\n",
        "    test_loss = 0.0  # Initialize test loss\n",
        "    correct = 0  # Initialize count of correct predictions\n",
        "    total = 0  # Initialize total count of predictions\n",
        "    self.model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Temporarily turn off gradient descent\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the DataLoader for test data\n",
        "        for i, data in enumerate(test_data):\n",
        "            inputs, target = data  # Get input and its corresponding groundtruth output\n",
        "            outputs = self.model(inputs)  # Get output from the model, given the inputs\n",
        "            loss = self.model.loss(outputs, target)  # Get loss for the predicted output\n",
        "            test_loss += loss.item()  # Update test loss\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels\n",
        "            total += target.size(0)  # Update total count of predictions\n",
        "            correct += (predicted == target).sum().item()  # Update count of correct predictions\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Log the accuracy to TensorBoard\n",
        "    self.writer.add_scalar('Accuracy/test', accuracy)\n",
        "\n",
        "    # Close the TensorBoard writer\n",
        "    self.writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5RpUCravHGy"
      },
      "source": [
        "# **Main Program**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gUk7UMkbzJmT"
      },
      "outputs": [],
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf /content/runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwpgcB2bDM0L"
      },
      "outputs": [],
      "source": [
        "# 1. Transform the data\n",
        "# Transforms to apply to the data - More about this later\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((120,90)),  # Uses bilinear resampling by default\n",
        "    transforms.Grayscale(), # Transform the image to grayscale to lower input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Loading the data\n",
        "rootPath = '/content/'\n",
        "csvPath = rootPath+\"english.csv\"\n",
        "dataset = ImageDataset(csvPath, transform)\n",
        "train_dataset = dataset.get_dataset('train')\n",
        "validate_dataset = dataset.get_dataset('validate')\n",
        "test_dataset = dataset.get_dataset('test')\n",
        "\n",
        "batch_size = 512\n",
        "# Initialize the training dataloader\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=1)\n",
        "# Initialize the validation dataloader\n",
        "val_loader = torch.utils.data.DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "# 2. The MLP model\n",
        "mlp_model = MLP(inputSize = 120*90, lr= 1e-03)\n",
        "\n",
        "# Load existing model\n",
        "if os.path.isfile(rootPath + 'model.pth'):\n",
        "    mlp_model.load_state_dict(torch.load('model.pth'))\n",
        "else:\n",
        "    print(\"No model file found.\")\n",
        "\n",
        "# 3. Training the network\n",
        "# 3.1. Creating the trainer class\n",
        "trainer = Trainer(n_epochs=24)\n",
        "\n",
        "# 3.2. Training the model\n",
        "trainer.fit(mlp_model, trainloader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1SYYPHAIe_w"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FELTDpi3DYMv"
      },
      "source": [
        "# **Testing the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAEGcspLI7p2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# printing some info about the dataset\n",
        "print(\"Number of Unique Values\")\n",
        "df = pd.read_csv(csvPath)\n",
        "classes =  sorted(df['label'].unique())\n",
        "print(classes)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Initialize the training dataloader\n",
        "batch_size = 16\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "# Initialize the tester\n",
        "tester = Tester(mlp_model)\n",
        "\n",
        "# Test the model\n",
        "tester.test(testloader)\n",
        "\n",
        "# Let's see some images\n",
        "for i, data in enumerate(testloader):\n",
        "    images, labels = data\n",
        "    imshow(torchvision.utils.make_grid(images))\n",
        "    print('GroundTruth: \\n', ' '.join(f'{classes[labels[j]]:5s}' for j in range(images.shape[0])))\n",
        "\n",
        "# Now, let's see what the network thinks these examples are\n",
        "    output = mlp_model(images)\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    print('EstimatedLabels: \\n', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(images.shape[0])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTKBCvRdIh09"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT2aBtbauB7L"
      },
      "source": [
        "# **Interations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U5b1oPzuITI"
      },
      "source": [
        "## **Interation 1**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: Using the original image size of 1200x900\n",
        "\n",
        "Learning Rate: 1e-04\n",
        "\n",
        "Number of hidden layers: 1\n",
        "\n",
        "Hidden layer 1's input/output size: (64, 32)\n",
        "\n",
        "Number of Epoch: 3\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The first run is aims to replicate lab 4 and there is little change other than starting off with the original size of the image\n",
        "\n",
        "The result has zero accuracy and due to the little number of epoches, it is difficult to determine whether model is over or underfitting\n",
        "\n",
        "Testing shows that it is always predicting the same letter for all test images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRCzVwPw1kPT"
      },
      "source": [
        "## **Iteration 2**\n",
        "\n",
        "Based on the previous attempt, I have decided to increase the number of Epoches.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 1200x900\n",
        "\n",
        "Learning Rate: 1e-04\n",
        "\n",
        "Number of hidden layers: 1\n",
        "\n",
        "Hidden layer 1's input/output size: (64, 32)\n",
        "\n",
        "Number of Epoch: 10\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The result is the same as interation 1, the model is zero accuracy in predicting any character.\n",
        "\n",
        "The loss curve shows no signs of overfitting or underfitting, the generalisation loss shows a flat ~4.1 for all epoches, while the training loss started at 7.8 before quickly dropping to ~4.1 but showing no signs of bouncing back during an overfit.\n",
        "\n",
        "This could mean that the model is underfitting but other hyperparameters are not handling the task well. Futher tuning is necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgi2VUZzL-st"
      },
      "source": [
        "## **Iteration 3**\n",
        "\n",
        "Based on the previous attempt, I have decided to increase the number and output size of the hidden layer.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 1200x900\n",
        "\n",
        "Learning Rate: 1e-04\n",
        "\n",
        "Number of hidden layers: 2\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 2's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 10\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Iteration attempt failed at training. RAM usage exceeded limit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t74u7sK-N7SV"
      },
      "source": [
        "## **Iteration 4**\n",
        "\n",
        "Based on the previous attempt, I have decided to decrease the input size from 1200x900 to 600x450.\n",
        "Decreasing the batch size would also help resolve the RAM usage issue but it is already taking a long time to training the model and I would prefer not to increase the training time more.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 600x450\n",
        "\n",
        "Learning Rate: 1e-04\n",
        "\n",
        "Number of hidden layers: 2\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 2's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 10\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now, this interation is more interesting that the previous few. The loss curve, while still clearly shows underfitting and the gradient is still steep, the accuracy showed some signs of improvement as the training went on.\n",
        "\n",
        "However, the accuacy is still far too low at a fraction of a single percent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSazeY3ld0MI"
      },
      "source": [
        "## **Iteration 5**\n",
        "\n",
        "Based on the previous attempt, I have decided to increase the learning rate, as the model clearly shows that it is learning but learns way too slowly. As such, we will slowly increase the learning rate for the next few interation and see if it helps.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 600x450\n",
        "\n",
        "Learning Rate: 1e-03\n",
        "\n",
        "Number of hidden layers: 2\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 2's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 10\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This time the model performed worst than before and is back to having absolute zero accuracy for all data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJPoq755kJzr"
      },
      "source": [
        "## **Iteration 6**\n",
        "\n",
        "As discussed previously, I am going to keep increasing the learning rate and see if things improve.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 600x450\n",
        "\n",
        "Learning Rate: 1e-02\n",
        "\n",
        "Number of hidden layers: 2\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 2's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 10\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Once again, I was met with a zero accuracy model. The model is still underfitting even though I have been increasing the learning rate.\n",
        "\n",
        "I find it hard to believe the choice activation function, loss function, or optimizer function itself could be the problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouae3o7IwJPD"
      },
      "source": [
        "## **Iteration 7**\n",
        "\n",
        "Looking at the previous attempts, I am going to try increasing the number of hidden layers instead. The learning rate will be set back to 1e-04\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 600x450\n",
        "\n",
        "Learning Rate: 1e-04\n",
        "\n",
        "Number of hidden layers: 3\n",
        "\n",
        "Hidden layer 1's input/output size: (512, 256)\n",
        "\n",
        "Hidden layer 2's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 3's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 10\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This interation showed much more improvements it has around 14% accuracy during testing, 8% during validation and 24% during training.\n",
        "\n",
        "However, the loss curves showed no signs of rebounding. This indicates that the model is still underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrF-JABP-DPE"
      },
      "source": [
        "## **Iteration 8**\n",
        "\n",
        "Looking at the previous attempts, I am keeping the current learning rate since it is working well. I am also not going to increase the number of hidden layers as I had almost maxed out the RAM given by Colab.\n",
        "There is also the issue of taking a long time for training.\n",
        "As such, I believe a further decrease in input size would be neccessary while increasing the number of epoch.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 300x225\n",
        "\n",
        "Learning Rate: 1e-04\n",
        "\n",
        "Number of hidden layers: 3\n",
        "\n",
        "Hidden layer 1's input/output size: (512, 256)\n",
        "\n",
        "Hidden layer 2's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 3's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 20\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In this interation, there are further improvements made to the model. Now, the model has 13% accuracy during testing, 19% accuracy during validation, and 33% accuracy during training.\n",
        "\n",
        "However, the training time is truely abyssmal thanks to the large number of nodes involved in this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_RLedxmSGYK"
      },
      "source": [
        "## **Iteration 9**\n",
        "\n",
        "Looking at the previous attempts successes, I am going to further increase the number of epoch with the goal of eventually reaching at least 80% accuracy for testing.\n",
        "Next, I will also try to further decrease the input size from 300x225 to 240x180 to help with the long training time.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 240x180\n",
        "\n",
        "Learning Rate: 1e-04\n",
        "\n",
        "Number of hidden layers: 3\n",
        "\n",
        "Hidden layer 1's input/output size: (512, 256)\n",
        "\n",
        "Hidden layer 2's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 3's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "For this interation, I got 18% accuracy for the test data, 54% accuracy for the training data, and 22% accuracy for the validation data.\n",
        "There were further improvements to the model but the progress felt slow and the accuracy for training data were increasing for more than validation or testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZGu4pclyhmu"
      },
      "source": [
        "## **Iteration 10**\n",
        "\n",
        "Looking at the previous attempts, I am going to attempt to slightly increase the learning rate from 1e-04 to 5e-04 and batch size from 64 to 128, hopefully it will help the model learn faster.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 240x180\n",
        "\n",
        "Learning Rate: 5e-04\n",
        "\n",
        "Number of hidden layers: 3\n",
        "\n",
        "Hidden layer 1's input/output size: (512, 256)\n",
        "\n",
        "Hidden layer 2's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 3's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This is embrassing but, I just realised before this interation that there was a mistake with my code. The label used for training was wrong and I had swapped the capital letters with small case letters.\n",
        "This cased my model to have been predicting incorrectly and probably explains why my training accuracy is way higher than my validation or testing accuracy.\n",
        "My model is currently having 30% accuracy for testing,\n",
        "60% accuracy for training and 40% for validation. Its loss curves are showing some signs that it might be overfitting soon. However, the model's performance is still poor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGmTPukXQ8Da"
      },
      "source": [
        "## **Iteration 11**\n",
        "\n",
        "Looking at the previous attempts, I am going to increase the number of hidden layers to 4 and further increased the number of epoch to 50.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 240x180\n",
        "\n",
        "Learning Rate: 5e-04\n",
        "\n",
        "Number of hidden layers: 4\n",
        "\n",
        "Hidden layer 1's input/outsize: (1024,512)\n",
        "\n",
        "Hidden layer 2's input/output size: (512, 256)\n",
        "\n",
        "Hidden layer 3's input/output size: (256, 128)\n",
        "\n",
        "Hidden layer 4's input/output size: (128, 64)\n",
        "\n",
        "Number of Epoch: 50\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The results were disappointing, even with 50 epoches and the additional layer, the model only had an accuracy of 35% for testing, 69% for training, and 42% for validation.\n",
        "\n",
        "I am now thinking that perhaps the output size of the last hidden layer is too small at 64 to make accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iteration 12**\n",
        "\n",
        "Looking at the previous attempts' results, I am going to decrease the number of hidden layers by removing the last hidden layer and decrease the number of epoch as well.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 240x180\n",
        "\n",
        "Learning Rate: 5e-04\n",
        "\n",
        "Number of hidden layers: 3\n",
        "\n",
        "Hidden layer 1's input/outsize: (1024,512)\n",
        "\n",
        "Hidden layer 2's input/output size: (512, 256)\n",
        "\n",
        "Hidden layer 3's input/output size: (256, 128)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The results for this model is 34% accuracy for testing, 62% accuracy for training and 41% accuracy for validation.\n",
        "While the end result is similar to the previous interactions, the loss curve showed signs of overfitting for the first time at epoch 26. Additionally, for the earlier training epoches, there were times where validation accuracy is higher than training accuracy.\n",
        "Maybe having a higher output for the last hidden layer allowed the model to better generalise what it learnt?"
      ],
      "metadata": {
        "id": "gIT0CgvI2kVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iteration 13**\n",
        "\n",
        "Looking at the previous attempts' results, I am going to further increase the output size of the last hidden layer and add a new hidden layer 1 with a higher input and output layer.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 240x180\n",
        "\n",
        "Learning Rate: 5e-04\n",
        "\n",
        "Number of hidden layers: 3\n",
        "\n",
        "Hidden layer 1's input/outsize: (2048, 1024)\n",
        "\n",
        "Hidden layer 2's input/outsize: (1024,512)\n",
        "\n",
        "Hidden layer 3's input/output size: (512, 256)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The results were 38% testin accuracy, 67% training accuracy, and 45% validation accuracy.\n",
        "The generalisation loss curve showed signs of overfitting at the 25th epoch.\n",
        "There isnt as much improvments to the model as I had hoped. Is recognizing handwritten characters really such an complex problem that it requires larger number of layers and nodes? Adding more layers and more nodes do not seems to improve the model in terms of performance either."
      ],
      "metadata": {
        "id": "JR50VoVXFTCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iteration 14**\n",
        "\n",
        "Looking at the previous attempts' results, increasing the number of hidden layers does not seems to increase the performance of the model much if at all. Increasing the output of the last hidden layer to 128 appears to be optimal as increasing it further does not increase performance and decreasing it decreases the model's performance, likely due to the model abstracting the input too much when the output is lowered.\n",
        "It is likely that I would be unable to train a MLP model that can recognise characters at high accuracy. Either due to MLP's inherent weakness in the task or due to the lack of training data.\n",
        "For the next few interations, I will instead try to decrease the required training time while trying to maintain the test accuracy of the model.\n",
        "I have decreased the number of hidden layers to 2 and decreased the input and output size of the layers as well.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 240x180\n",
        "\n",
        "Learning Rate: 1e-03\n",
        "\n",
        "Number of hidden layers: 2\n",
        "\n",
        "Hidden layer 1's input/outsize: (516,256)\n",
        "\n",
        "Hidden layer 2's input/output size: (256, 128)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The results were 27% test accuracy, 46% training accuracy, and 33% validation accuracy.\n",
        "There were little signs of overfitting, looking at the loss curves."
      ],
      "metadata": {
        "id": "T4yaOyLec4t_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iteration 15**\n",
        "\n",
        "In this interation, I have further removed more hidden layers and leaving only a single hidden layer with input/output size of (256,128).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 240x180\n",
        "\n",
        "Learning Rate: 1e-03\n",
        "\n",
        "Number of hidden layers: 1\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The results were 29% test accuracy, 48% training accuracy, and 36% validation accuracy.\n",
        "\n",
        "Considering the number of hidden layers and the relatively small input and output size of the hidden layer, I believe this set of hyperparameters is doing really well when even the best model so far still has less than 40% test accuracy but with a much larger number of hidden layers and input/output size."
      ],
      "metadata": {
        "id": "frTmb8huYdSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iteration 16**\n",
        "\n",
        "In this interation, I have reduced the input size of the image by resizing it to 120x90 while keeping the rest of the hyperparameter the same.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 240x180\n",
        "\n",
        "Learning Rate: 1e-03\n",
        "\n",
        "Number of hidden layers: 1\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The results were 31% test accuracy, 57% training accuracy, and 44% validation accuracy.\n",
        "Decreasing the resolution of the image seems to slightly increase the performance of the model, it could just be by chance. However, it is quite clear that it did not negatively affect the performance despite the decrease in image resolution."
      ],
      "metadata": {
        "id": "pvur0h7n1T2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iteration 17**\n",
        "\n",
        "In this interation, I have further decreased the input size of the images to 120x90 and increased the batch size from 256 to 512 as an attempt to decrease the training time required.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 120x90\n",
        "\n",
        "Learning Rate: 1e-03\n",
        "\n",
        "Number of hidden layers: 1\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The result is 30% test accuracy, 47% training accuracy, and 37% validation accuracy.\n",
        "While there is a significant decrease in training and validation accuracy. There is very little decrease in test accuracy.\n",
        "Looking at the loss curve, there are signs where the model looks to be begining to overfit near the later epoches."
      ],
      "metadata": {
        "id": "Dt2Hipzg2kK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iteration 18**\n",
        "\n",
        "In this interation, I have further decreased the input size of the images to 60x45\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 60x45\n",
        "\n",
        "Learning Rate: 1e-03\n",
        "\n",
        "Number of hidden layers: 1\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Number of Epoch: 30\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The results were 29% test accuracy, 43% training accuracy, and 38% validation accuracy.\n",
        "There were little loss in accuracy despite another decrease in image resolution.\n",
        "The loss curve this time did not show signs of overfitting."
      ],
      "metadata": {
        "id": "e0cS7CJVkmiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Model Selection**\n",
        "\n",
        "I have chosen iteration 17's hyperparameter to be the model of choice. The main reasons are that it has a relatively high performance for the low training time required, thanks to the small image resolution and small number of hidden layers.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Input size: 120x90\n",
        "\n",
        "Learning Rate: 1e-03\n",
        "\n",
        "Number of hidden layers: 1\n",
        "\n",
        "Hidden layer 1's input/output size: (256, 128)\n",
        "\n",
        "Number of Epoch: 24\n",
        "\n",
        "Activation function: ReLU\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "\n",
        "Optimizer function: Adam\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RHDkRxAh7ti-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extension to Localise and recognise multiple characters in an Image**"
      ],
      "metadata": {
        "id": "-JEmou9RH8lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing some info about the dataset\n",
        "print(\"Number of Unique Values\")\n",
        "df = pd.read_csv(csvPath)\n",
        "classes =  sorted(df['label'].unique())\n",
        "\n",
        "# Load the trained MLP model\n",
        "mlp_model = MLP(inputSize = 120*90, lr= 1e-03)  # Initialize the MLP model\n",
        "mlp_model.load_state_dict(torch.load(rootPath + 'model.pth'))  # Load the trained weights\n",
        "mlp_model.eval()\n",
        "\n",
        "# Load and preprocess the image\n",
        "image = cv2.imread(rootPath + 'multi-character.png', cv2.IMREAD_GRAYSCALE)\n",
        "_, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "# Find contours in the binary image\n",
        "contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# For each contour, get the bounding box and recognize the character\n",
        "for contour in contours:\n",
        "    x, y, w, h = cv2.boundingRect(contour)\n",
        "    cropped_image = image[y:y+h, x:x+w]\n",
        "    resized_image = cv2.resize(cropped_image, (90, 120))  # Resize to the input size of the MLP model\n",
        "\n",
        "    # Display the localized character\n",
        "    plt.imshow(resized_image, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    # Convert to tensor and add batch and channel dimensions\n",
        "    cropped_image_tensor = torch.from_numpy(resized_image).float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Recognize the character with the MLP model\n",
        "    with torch.no_grad():\n",
        "        output = mlp_model(cropped_image_tensor)\n",
        "        _, predicted_label = torch.max(output, 1)\n",
        "        predicted_character = classes[predicted_label.item()]\n",
        "        print(f'The predicted character is: {predicted_character}')"
      ],
      "metadata": {
        "id": "wo_nVuoZUhKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}